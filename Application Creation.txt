I am a AWS Cloud DevOps engineer working on a task. Imagine you are me and you have to complete this task. My constraints are that I must use the following AWS technologies:
AWS Lambda, AWS EventBridge, Amazon S3 and AWS SDK for Python
I need to develop an application written in Python. The application must collect metrics from the number and overall size of unattached disk volumes, not ecnrypted disk volumes and not encrypted snapshots and store the metrics as JSON files.
I need help with generating the code for this python application. I also need help with generating infrastructure using python AWS SDK. The following infrastructure must be created:
1 AWS Lambda, 1 EventBridge trigger that will trigger our Lambda function once a day, 1 unattached disk, 2 non-encrypted disks, 3 encrypted snapshots, 1 S3 bucket that will store the result of the python application, IAM Role and permissions attached to that role so that the application may query AWS for needed metrics and store them in S3.
Please split the python files in separate .py files one for setting up infrastructure and one for the main application. Please also give me the commands to setup a new python venv and generate a requirements.txt I should upload.
Please also keep into consideration that I am working under MacOS.
Let me know if you need any clarifying details before you can begin.
ChatGPT

Letâ€™s break this task down step-by-step:

    Environment Setup:

    bash

# Create a virtual environment
python3 -m venv aws-metrics-env

# Activate the environment
source aws-metrics-env/bin/activate

# You will need to install the required packages
pip install boto3

# Generate requirements.txt
pip freeze > requirements.txt

Infrastructure Setup Code (setup_infrastructure.py):

python

import boto3

def create_s3_bucket(bucket_name):
    s3 = boto3.resource('s3')
    s3.create_bucket(Bucket=bucket_name)

def create_disks_and_snapshots():
    ec2 = boto3.resource('ec2')
    # Create unattached disk
    ec2.create_volume(Size=10, AvailabilityZone='us-west-1a')
    
    # Create non-encrypted disks
    for _ in range(2):
        ec2.create_volume(Size=10, AvailabilityZone='us-west-1a', Encrypted=False)
    
    # Create encrypted snapshots
    for _ in range(3):
        volume = ec2.create_volume(Size=10, AvailabilityZone='us-west-1a', Encrypted=True)
        snapshot = volume.create_snapshot(Description="Encrypted Snapshot")
        # If you want to cleanup and not maintain the volumes, delete them after creating snapshots
        volume.delete()

def create_lambda_role():
    iam = boto3.client('iam')
    trust_relationship = {
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Principal": {"Service": "lambda.amazonaws.com"},
            "Action": "sts:AssumeRole"
        }]
    }
    response = iam.create_role(RoleName='LambdaMetricsRole', AssumeRolePolicyDocument=json.dumps(trust_relationship))
    role_arn = response['Role']['Arn']
    
    # Attach necessary permissions to the role
    policies = ["arn:aws:iam::aws:policy/AmazonS3FullAccess", "arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess"]
    for policy in policies:
        iam.attach_role_policy(RoleName='LambdaMetricsRole', PolicyArn=policy)
    
    return role_arn

if __name__ == "__main__":
    bucket_name = "aws-metrics-bucket"
    create_s3_bucket(bucket_name)
    create_disks_and_snapshots()
    create_lambda_role()

Python Application (app.py):

python

    import boto3
    import json

    def lambda_handler(event, context):
        ec2 = boto3.resource('ec2')
        s3 = boto3.resource('s3')
        
        unattached_volumes = list(ec2.volumes.filter(Filters=[{'Name': 'status', 'Values': ['available']}]))
        non_encrypted_volumes = list(ec2.volumes.filter(Filters=[{'Name': 'encrypted', 'Values': ['false']}]))
        non_encrypted_snapshots = list(ec2.snapshots.filter(Filters=[{'Name': 'encrypted', 'Values': ['false']}]))
        
        metrics = {
            'unattached_volumes_count': len(unattached_volumes),
            'unattached_volumes_size': sum([vol.size for vol in unattached_volumes]),
            'non_encrypted_volumes_count': len(non_encrypted_volumes),
            'non_encrypted_snapshots_count': len(non_encrypted_snapshots)
        }

        # Save metrics to S3
        bucket_name = "aws-metrics-bucket"
        s3.Object(bucket_name, 'metrics.json').put(Body=json.dumps(metrics))

        return {
            'statusCode': 200,
            'body': json.dumps(metrics)
        }

    Deploy the Lambda Function:

    You can zip the app.py file and use AWS Management Console or AWS CLI to create the Lambda function and provide the appropriate IAM role created by the setup_infrastructure.py script.

    EventBridge Configuration:

    Once the Lambda function is deployed, go to the AWS EventBridge console, create a new rule, specify a daily schedule, and set the target as the Lambda function.

Note:

    Ensure that you've specified the right region (us-west-1a is used in the example, but you might have to adjust).
    The above scripts are simplified and may require error-handling, retries, etc. for production use.
    Before executing scripts, ensure you've configured AWS CLI with necessary credentials and default region.
    Always ensure you're not violating AWS service limits or incurring unnecessary costs.
    Ensure that all necessary IAM permissions are in place to run the scripts and functions.
    The Lambda function may need tweaking depending on your deployment method (e.g., if you use deployment packages, layers, or SAM).

User
How would I run the setup_infrastructure.py file? Should I generate a key pair from AWS IAM to export in environment variables and them pass them somehow in AWS SDK to authorize against AWS to provision these resources?
ChatGPT

Running setup_infrastructure.py involves authenticating against AWS to provision the specified resources. Here's a step-by-step guide:

    Setting up AWS Credentials:

    You should have the AWS CLI installed and configured with the necessary access rights. If not:

    bash

pip install awscli
aws configure

This will prompt you for:

    AWS Access Key ID
    AWS Secret Access Key
    Default region name
    Default output format

These credentials will be saved in ~/.aws/credentials and ~/.aws/config respectively.

Alternatively, you can use environment variables:

bash

export AWS_ACCESS_KEY_ID='your_access_key'
export AWS_SECRET_ACCESS_KEY='your_secret_key'
export AWS_DEFAULT_REGION='your_region'

Note: Ensure the IAM user associated with these credentials has the necessary permissions to create EC2 volumes, snapshots, S3 buckets, and IAM roles.

Running the script:

Activate your virtual environment if it's not active:

bash

source aws-metrics-env/bin/activate

Execute the script:

bash

    python setup_infrastructure.py

    Authorization:

    When you run AWS SDK commands, it first looks for credentials in environment variables (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY). If not found, it will then look for the ~/.aws/credentials file created by aws configure. As such, you don't need to explicitly pass these credentials in your Python code.

Security Best Practices:

    Avoid hardcoding credentials in your scripts or applications.
    Use IAM roles with the least privilege principle. Grant only permissions that are necessary for the tasks.
    Consider using IAM roles for EC2 or other services if running this on AWS services to avoid managing keys.
    Rotate keys regularly and use IAM policies to enforce this.
    Always be careful not to accidentally commit and push AWS credentials to public repositories.

Lastly, when provisioning infrastructure programmatically, consider infrastructure as code tools like AWS CloudFormation or AWS CDK, which provide more structured, maintainable, and repeatable methods of deploying resources.